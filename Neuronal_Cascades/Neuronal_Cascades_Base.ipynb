{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gudhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuron():\n",
    "    \"\"\"\n",
    "    A `neuron` object. This class is the building blocks of the `Geometric_Brain_Network`. This class is designed\n",
    "    in a way that each `neuron` can be given special intrinsic properties, instead of global rules. This \n",
    "    distinction enables to model neurons that are differing in functionality and to include heterogeneity in \n",
    "    the complex spreads we run on the networks compounded by them.\n",
    "    \n",
    "    Attributes\n",
    "    ================\n",
    "    \n",
    "    neuron.id: int\n",
    "        The id of a neuron.\n",
    "        \n",
    "    neuron.state: int\n",
    "        The state of a neuron which is either on (1), off (0) or rest (-1). \n",
    "        \n",
    "        If a neuron is on (1) at a given time step, it is going to contribute to it's neighbors' activation \n",
    "        who are inactive(0). If a neuron is off (0), it is ready to take input from it's active neighbors(1) i.e.\n",
    "        it is ready to fire. If a neuron is resting (-1), it is neither contributing to it's neighbors' activation\n",
    "        or it is able to receive input from it's active neighbors.\n",
    "        \n",
    "    neuron.memory: int\n",
    "        Number of time points that the neuron is going to stay on (1) once it's activated i.e how long a neuron\n",
    "        is going to remember it's activation enabling us modeling higher order Markov processes. Default is 0.\n",
    "        \n",
    "        A neuron with 'bad memory', memory of length 0, can not remember it's past, it can only remember the \n",
    "        current state. A neuron with memory of length 1, can remember it's activation for only 1 step in addition\n",
    "        to the step where it is active total of two steps of activation. A neuron with 'full memory' remembers \n",
    "        it's activation throughout the whole experiment i.e. once it's activated it never gets deactivated. \n",
    "        \n",
    "    neuron.rest: int\n",
    "        Number of time points that the neuron is going to stay off (0) once it's dectivated i.e. refractory \n",
    "        period. Default is 0. \n",
    "        \n",
    "        Inspired by the neurons in the nervous system, every `neuron` will have a resting period that is \n",
    "        relatively longer than the action potentials. So, once a neuron comes to the end of it's memory, it is \n",
    "        going to rest for some amount of time i.e. it will not be able to contribute to the system or it will not \n",
    "        be able to recieve inputs from it's neighbors. An 'energetic neuron', `neuron` with rest 0, never pauses \n",
    "        and it is either on (1) or off(0) whereas a 'lazy neuron', `neuron` with rest > 0 pauses to receive input \n",
    "        once it's deactivated rest many time steps.\n",
    "        \n",
    "    neuron.threshold: float, in [0,1]\n",
    "        We determine if a neuron is going to fire or not according to a simple rule: if the difference between\n",
    "        the ratio of its active neighbors to all neighbors is big enough than the `threshold`, it fires. \n",
    "        \n",
    "        This value is intrinsic to every `neuron` and mainly determines firing patterns and frequencies of \n",
    "        neurons.\n",
    "        \n",
    "    neuron.history: list\n",
    "        Every `neuron` keeps track of it's state per se throughout an experiment i.e. it's history. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, state = False, memory = 0, rest = 0, threshold = 0.1):\n",
    "        self.id = name\n",
    "        self.state = state\n",
    "        self.memory = memory\n",
    "        self.rest = rest\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        self.refresh_history()\n",
    "        \n",
    "    def refresh_history(self):\n",
    "        \"\"\"\n",
    "        Helper method to refresh the history of a neuron back to empty list. Neccessary for consecutive calls\n",
    "        of the dynamics as in `make_distance_matrix`.\n",
    "        \"\"\"\n",
    "        self.history = []\n",
    "        \n",
    "        \n",
    "class Geometric_Brain_Network():\n",
    "    \"\"\"\n",
    "    This class is the main object we run our contagions on. `Geometric_Brain_Netwok` object explicitly inputs\n",
    "    the `neuron` objects. The topology of the network is one of the important features of the network. We chose 2\n",
    "    and 3 dimensional compact manifolds as our fundamental network topology pool for now. The importance of \n",
    "    geometric networks is that the geometric degree is a global property instead of a local property, same with \n",
    "    non-geometric degree. So, the total degree of every node is GD + nGD.\n",
    "    \n",
    "    Attributes\n",
    "    ===============\n",
    "    Geometric_Brain_Network.N: int\n",
    "        Number of nodes in the network.\n",
    "        \n",
    "    Geometric_Brain_Network.neurons: list\n",
    "        A list of `neuron` objects. Length of the list should match with number the of the nodes. See \n",
    "        `Geometric_Brain_Network.get_neurons`.\n",
    "        \n",
    "    Geometric_Brain_Network.GD: int\n",
    "        Geometric degree of a node. Geometric degree is the number of 'closest' nodes(in a euclidean distance \n",
    "        manner) to connect to a node. For `Ring` topology, it has to be an even number because close neighbors \n",
    "        are located both on the left and right from a node. It also\n",
    "        \n",
    "    Geometric_Brain_Network.nGD: int\n",
    "        Non-geometric degree of a node. Non-geometric edges are randomly connected to a node that are 'distant'.\n",
    "        The combination of these two types of edges enables to types of phenomenan to occur, wavefront \n",
    "        propagation(WFP) and appearance of new clusters(ANC).\n",
    "        \n",
    "    Geometric_Brain_Network.manifold: string\n",
    "        The network topology that the nodes are sitting on. This type determines the manifold that the WFP is \n",
    "        going to follow. Also, the persistent homology of the first activation of the nodes during an experiment\n",
    "        recovers this type of topology as well. \n",
    "        \n",
    "        This can be 'Ring', that is the only compact 1D-manifold, 'Sphere' and 'Torus', only compact 2D-manifolds,\n",
    "        for now. In the case of 'Sphere' and 'Torus', `geometric_degree` is not possible to be distributed\n",
    "        uniformly among nodes, so some nodes may have slightly higher GD in these options but on average, \n",
    "        geometric degree is going to be close to GD which you can see using `self.text`. Also, in the case of '\n",
    "        Torus', because of the parametrization we use, the initial number of nodes should be a square.\n",
    "        \n",
    "    Geometric_Brain_Network.A: sparse matrix\n",
    "        The adjacency matrix of the network keeping track of the neighborhood information.\n",
    "        \n",
    "    Geometric_Brain_Network.time: int\n",
    "        Initialized from 0, see `get_neurons`. When we run a contagion on the network time will be iterated \n",
    "        accordingly.\n",
    "    \n",
    "    Geometric_Brain_Network.text: str\n",
    "        A handy summary of the network for figures and whatnot.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size, geometric_degree = 1, nongeometric_degree = 0, manifold = 'Ring'):\n",
    "        \n",
    "        self.N = size  \n",
    "        self.GD = geometric_degree\n",
    "        self.nGD = nongeometric_degree\n",
    "        self.A = sparse.lil_matrix(((self.N, self.N)), dtype = bool)\n",
    "        self.manifold = manifold\n",
    "        self.text = '%s Network on %d nodes'%(self.manifold, self.N)\n",
    "        \n",
    "        self.make_geometric()\n",
    "        if self.nGD > 0: self.add_noise_to_geometric()\n",
    "            \n",
    "    def get_neurons(self, neurons):\n",
    "        \"\"\"\n",
    "        Method to append the `neuron` objects into the network. We don't initialize the network with neurons in \n",
    "        case we need to use the same network for different neuronal properties because we don't have control over \n",
    "        the randomly added non-geometric edges in the work. So, we can just add a new set of neurons to the \n",
    "        network by using this method.\n",
    "        \n",
    "        Parameters\n",
    "        ===========\n",
    "        neurons: list\n",
    "            A list of `neuron` objects of length `self.N`. \n",
    "        \"\"\"\n",
    "        if self.N != len(neurons): \n",
    "            raise InputError('Number of neurons provided should be size %d'%self.N)\n",
    "        else:\n",
    "            self.nodes = neurons\n",
    "        \n",
    "    def make_geometric(self):\n",
    "        \"\"\"\n",
    "        Method that is called automatically during initialization. Creates a `self.manifold` type of topology \n",
    "        based on the geometric degree and size.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.manifold == 'Ring':\n",
    "            \n",
    "            if self.GD >= int(self.N)-1: \n",
    "                raise InputError('Geometric Degree cannot exceed the half of the size of the network.')\n",
    "            elif self.GD<1 or self.GD%2 == 1:\n",
    "                raise InputError('Geometric Degree should be an even positive integer.')\n",
    "            \n",
    "            GD = int(self.GD/2)\n",
    "            for u in range(self.N):\n",
    "                for i in range(1, GD + 1):\n",
    "                    #from left\n",
    "                    if u + i >= self.N: \n",
    "                        v = u + i - self.N\n",
    "                    else: v = u + i\n",
    "                    self.A[u,v] = True\n",
    "                    #from right\n",
    "                    if u - i < 0: \n",
    "                        v = self.N + u - i\n",
    "                    else: v = u - i\n",
    "                    self.A[u,v] = True\n",
    "            self.text = self.text + ' w/ GD %d'%(self.GD)\n",
    "            \n",
    "        elif self.manifold == 'Sphere': \n",
    "            ## We use the method called 'Golden Spiral' to evenly distribute n points on a sphere.\n",
    "            \n",
    "            indices = np.arange(0, self.N, dtype=float) + 0.5\n",
    "\n",
    "            phi = np.arccos(1 - 2*indices/self.N)\n",
    "            theta = np.pi * (1 + 5**0.5) * indices\n",
    "\n",
    "            x, y, z = np.cos(theta) * np.sin(phi), np.sin(theta) * np.sin(phi), np.cos(phi)\n",
    "            \n",
    "            closest = np.argsort(distance.squareform(distance.pdist(np.array([list(x), \n",
    "                                                                              list(y), \n",
    "                                                                              list(z)]).T, \n",
    "                                                                    'euclidean')), axis=1)\n",
    "            for i, e in enumerate(closest[:,1:self.GD+1]):\n",
    "                for j, f in enumerate(e):\n",
    "                    self.A[i,f] = True\n",
    "                    self.A[f,i] = True\n",
    "                    \n",
    "            self.text = self.text + ' w/ average GD %.2f'%(np.sum(self.A.toarray())/self.N)\n",
    "                    \n",
    "            #plt.figure().add_subplot(111, projection='3d').scatter(x, y, z);\n",
    "            #plt.show()\n",
    "        \n",
    "        elif self.manifold == 'Torus':\n",
    "            if int(sqrt(self.N))**2 != self.N : \n",
    "                raise InputError('Number of nodes should be a square(36, 100, 900 etc..) with Torus topology.')\n",
    "\n",
    "            angle = np.linspace(0, 2 * np.pi, int(sqrt(self.N)))\n",
    "            theta, phi = np.meshgrid(angle, angle)\n",
    "            r, R = 5, 10.\n",
    "            X = (R + r * np.cos(phi)) * np.cos(theta)\n",
    "            Y = (R + r * np.cos(phi)) * np.sin(theta)\n",
    "            Z = r * np.sin(phi)\n",
    "\n",
    "            closest = np.argsort(distance.squareform(distance.pdist(np.array([list(X.reshape(-1)), \n",
    "                                                                              list(Y.reshape(-1)), \n",
    "                                                                              list(Z.reshape(-1))]).T, \n",
    "                                                                    'euclidean')), axis=1)\n",
    "            for i, e in enumerate(closest[:,1:self.GD+1]):\n",
    "                for j, f in enumerate(e):\n",
    "                    self.A[i,f] = True\n",
    "                    self.A[f,i] = True\n",
    "            \n",
    "            self.text = self.text + ' w/ average GD %.2f'%(np.sum(self.A.toarray())/self.N)\n",
    "            \n",
    "            #plt.figure().add_subplot(111, projection='3d').scatter(X, Y, Z, s = 1);\n",
    "            #plt.show()\n",
    "        \n",
    "    def add_noise_to_geometric(self):\n",
    "        \"\"\"\n",
    "        Method that is called automatically as long as non-geometric degree is non-zero.\n",
    "        \"\"\"\n",
    "\n",
    "        M = int(self.N * self.nGD)\n",
    "        \n",
    "        if M%2 == 1: raise ValueError('Try providing an even non-geometric degree')\n",
    "            \n",
    "        flag_2 = True\n",
    "            \n",
    "        while flag_2:\n",
    "            flag_2 = False\n",
    "            #build stubs\n",
    "            stubs = np.zeros(M)\n",
    "            for i in range(self.N):\n",
    "                index = (i*self.nGD) + np.arange(self.nGD)\n",
    "                stubs[index[0]:index[-1]+1] = (i) * np.ones(self.nGD)\n",
    "                    \n",
    "            #build undirected link list\n",
    "            link_list = np.zeros((int(M/2),2))\n",
    "            for m in range(int(M/2)):\n",
    "                flag_1 = True # turn on flag to enter while loop\n",
    "                count = 0\n",
    "                while flag_1:\n",
    "                    flag_1 = False #turn off flag to exit while loop\n",
    "                    rand = [random.randint(0, len(stubs)-1) for i in range(2)]\n",
    "                    \n",
    "                    node_A = int(stubs[rand[0]])\n",
    "                    node_B = int(stubs[rand[1]])\n",
    "                                            \n",
    "                    if node_A == node_B: flag_1 = True\n",
    "                    \n",
    "                    for n in range(m):\n",
    "                        if link_list[n,0] == node_A and link_list[n,1] == node_B:\n",
    "                            flag_1 = True\n",
    "                        if link_list[n,0] == node_B and link_list[n,1] == node_A:\n",
    "                            flag_1 = True\n",
    "                        if self.A.toarray()[node_A][node_B] == 1 or self.A.toarray()[node_B][node_A] == 1:\n",
    "                            flag_1 = True\n",
    "                            \n",
    "                    count = count +1\n",
    "                    \n",
    "                    if count > M: flag_2 = True ; break\n",
    "                        \n",
    "                #make link       \n",
    "                link_list[m,0] = node_A\n",
    "                link_list[m,1] = node_B\n",
    "                \n",
    "                #remove stubs from list\n",
    "                stubs = np.delete(stubs,[rand[0],rand[1]])\n",
    "        \n",
    "        #build network\n",
    "        for m in range(int(M/2)):\n",
    "            self.A[link_list[m,0],link_list[m,1]] = True\n",
    "            self.A[link_list[m,1],link_list[m,0]] = True\n",
    "        self.text = self.text + ' and nGD %d'%self.nGD\n",
    "        \n",
    "    def neighbors(self, node_id):\n",
    "        \"\"\"\n",
    "        Helper function to get the neighbors of a node.\n",
    "        \n",
    "        Parameters\n",
    "        =============\n",
    "        node_id: int  \n",
    "            Id of the `neuron` to call the neighbors of.\n",
    "        \n",
    "        Returns\n",
    "        =========\n",
    "        array:  self.A.toarray()[node.id].nonzero()[1], 1 x k\n",
    "            Containing the neighbor indices where k = r'$GD + nGD$.\n",
    "        \n",
    "        \"\"\"\n",
    "        return(self.A.todense()[node_id].nonzero()[1])\n",
    "    \n",
    "    def neighbor_input(self, node_id):\n",
    "        \"\"\"\n",
    "        Helper function to compute the incoming activation. It first gets the neighbors of the node, then finds \n",
    "        the active ones among them. Then, it computes how much the ratio of active neighbors to the total \n",
    "        neighbors differ from the `threshold` of the node.\n",
    "        \n",
    "        Parameters\n",
    "        ===========\n",
    "        node_id: int\n",
    "            Id of the neuron to get the difference of the incoming activation from the `threshold`.\n",
    "    \n",
    "        Returns\n",
    "        =========\n",
    "        F: float\n",
    "        \"\"\"\n",
    "        nbhood = self.neighbors(node_id)\n",
    "        active_hood = []\n",
    "        for i,e in enumerate(nbhood):\n",
    "            if self.nodes[e].state == 1:\n",
    "                active_hood.append(e)\n",
    "                \n",
    "        F = len(active_hood)/len(nbhood) - self.nodes[node_id].threshold\n",
    "        return(F)\n",
    "    \n",
    "    def sigmoid(self, node_id, C):\n",
    "        \"\"\"\n",
    "        The step function we use is a sigmoid that is r$\\sigma(x) = \\dfrac{1}{1+e^{-C*x}}$. Most \n",
    "        models use either a discontinous or non smooth, linear step functions. We use this step function to have\n",
    "        more control on the stochasticty of the model. Explicitly, the parameter `C` controls the curvature of S.\n",
    "        The bigger the `C` is the less curvy the sigmoid gets i.e. system tends to be deterministic. The smaller \n",
    "        `C` gets, system will inherit stochasticity.\n",
    "        \n",
    "        Parameters\n",
    "        =============\n",
    "        node_id: int\n",
    "            Id of the neuron to determine if it's going to fire or not.\n",
    "        C: int\n",
    "            Curvature of sigmoid.\n",
    "        Returns\n",
    "        ========\n",
    "        Z: float\n",
    "        \"\"\"\n",
    "        F = self.neighbor_input(node_id)\n",
    "        Z = 1/(1+np.exp(-C*F))\n",
    "        return(Z)\n",
    "    \n",
    "    def update_history(self, node_id, C):\n",
    "        \"\"\"\n",
    "        One of the two core methods. Specifically, `update_history` is called at each iteration to update the\n",
    "        history of the neurons. Note that history is a property of the `neuron` objects. The function first calls\n",
    "        the `sigmoid` which inherently calls the `neighbor_input` to compute the scalar that the system will \n",
    "        determine if the `neuron` in hand is going to fire or not. A random number is generated from a uniform \n",
    "        distribution and the truth value of the comparison between the random number and scalar gets assigned as \n",
    "        the state of the node. If `C` is large no matter the random number is, this truth value is going to be \n",
    "        `True` if the scalar is positive and is going to be `False` if the scalar is negative i.e. when the system \n",
    "        is deterministic. If the truth value is `True`, 1 is going to be appended to the history of the neuron \n",
    "        followed by `memory` of the node many 1's and `rest` many -1's. Additionaly, a single 0 will be appended to\n",
    "        the history to make the neuron ready to fire once it's cycle is complete. If the truth value is `False`, a \n",
    "        0 is going to be appended to the history.\n",
    "        \n",
    "        Parameters\n",
    "        ===============\n",
    "        node_id: int\n",
    "            Id of the node at hand to be updated.\n",
    "        C: int\n",
    "            Curvature of the sigmoid.\n",
    "        \"\"\"\n",
    "        \n",
    "        rand = random.uniform(0,1)\n",
    "        \n",
    "        if rand <= self.sigmoid(node_id, C):\n",
    "            \n",
    "            for i in range(self.nodes[node_id].memory + 1):\n",
    "                self.nodes[node_id].history.append(1)\n",
    "                \n",
    "            for i in range(self.nodes[node_id].rest):\n",
    "                self.nodes[node_id].history.append(-1)\n",
    "                \n",
    "            self.nodes[node_id].history.append(0)\n",
    "            \n",
    "        else:\n",
    "            self.nodes[node_id].history.append(0)\n",
    "    \n",
    "    def update_states(self):\n",
    "        \"\"\"\n",
    "        One of the two core methods of the contagion model. Goes through all of the nodes and updates the \n",
    "        current state of the node according to the current `time` of the network. Also, classifies the subset \n",
    "        of nodes according to their states during this pass of all nodes.\n",
    "        \n",
    "        Returns\n",
    "        ========\n",
    "        excited: list\n",
    "            List of node ids that are excited (1) at the current `time`.\n",
    "        ready_to_fire: list\n",
    "            List of node ids that are ready to fire (0) at the current `time`.\n",
    "        rest: list\n",
    "            List of node ids that are resting at the current `time`.\n",
    "        \n",
    "        \"\"\"\n",
    "        excited = []\n",
    "        ready_to_fire = []\n",
    "        rest = []\n",
    "        \n",
    "        for node in self.nodes:\n",
    "            \n",
    "            node.state = int(node.history[self.time])\n",
    "                \n",
    "            if node.state == 1:\n",
    "                excited.append(node.id)\n",
    "            elif node.state == 0:\n",
    "                ready_to_fire.append(node.id)\n",
    "            else: rest.append(node.id)\n",
    "                \n",
    "        return(excited, ready_to_fire, rest)\n",
    "                \n",
    "    def initial_spread(self, seed):\n",
    "        \"\"\"\n",
    "        Helper function to start the contagion. Same as `update_history` except that at the beginning, at `time=0`\n",
    "        we want all neighbors of the seed node to be activated (1) with probability 1 and all other nodes to be\n",
    "        deactivated (0).\n",
    "        \"\"\"\n",
    "        all_nodes = set([k for k in range(self.N)])\n",
    "        excited_nodes = self.neighbors(seed)\n",
    "        \n",
    "        for node in excited_nodes:\n",
    "            for i in range(self.nodes[node].memory+1):\n",
    "                self.nodes[node].history.append(1)\n",
    "            for i in range(self.nodes[node].rest):\n",
    "                self.nodes[node].history.append(-1)\n",
    "                \n",
    "            self.nodes[node].history.append(0)\n",
    "            \n",
    "        for node in list(all_nodes-set(excited_nodes)):\n",
    "            self.nodes[node].history.append(0)\n",
    "            \n",
    "    def refresh(self):\n",
    "        \"\"\"\n",
    "        Helper method to refresh the `neuron.history` for each neuron as well as to set the internal network time\n",
    "        back to 0. Necessary for consecutive calls of the `run_dynamic`.\n",
    "        \n",
    "        Returns\n",
    "        ========\n",
    "        tolerence:int\n",
    "            To make the algorithm more efficient, system breaks if the tolerence exceeds some number. We set it\n",
    "            equal to 0 every time a new dynamic runs.\n",
    "        \"\"\"\n",
    "        self.time = 0\n",
    "        tolerence = 0\n",
    "        for node in self.nodes:\n",
    "            node.refresh_history()\n",
    "        return(tolerence)\n",
    "            \n",
    "    def run_dynamic(self, seed, TIME, C):\n",
    "        \"\"\"\n",
    "        The main iterator that calls the core methods and keeps track of the first activations as wells as the \n",
    "        size of the active component (1) of the network. Every time this function is called, the time of the \n",
    "        network will be set back to 0.\n",
    "        \n",
    "        Parameters\n",
    "        ==============\n",
    "        seed: int\n",
    "            Node id of the node that the contagion is going to start from.\n",
    "            \n",
    "        TIME: int\n",
    "            The number of time steps that the experiment is going to run for.\n",
    "            \n",
    "        C: int\n",
    "            Curvature of the sigmoid.\n",
    "        \n",
    "        Returns\n",
    "        ===========\n",
    "        activation_times: array of size `1 x N`\n",
    "            Array keeping track of the first times that a given node is activated i.e. duration it takes for \n",
    "            spread starting from the seed to reach every other node.\n",
    "            \n",
    "        size_of_contagion: array of size `1 x TIME`\n",
    "            Array saving the length of active nodes at every time step.\n",
    "        \n",
    "        \"\"\"\n",
    "        tolerence = self.refresh()\n",
    "        activation_times = np.ones(self.N, dtype = int)*TIME\n",
    "        size_of_contagion = [int(0)]\n",
    "        \n",
    "        self.initial_spread(seed)\n",
    "        excited_nodes, ready_to_fire_nodes, resting_nodes = self.update_states()\n",
    "        \n",
    "        self.time = 1\n",
    "        \n",
    "        while self.time < TIME and 0 < len(excited_nodes) and tolerence < 5 and np.any(activation_times==TIME):\n",
    "            size_of_contagion.append(len(excited_nodes))\n",
    "            \n",
    "            activation_times[excited_nodes] = np.minimum(activation_times[excited_nodes], \n",
    "                                                         np.array([self.time]*len(excited_nodes)))\n",
    "            \n",
    "            for node in ready_to_fire_nodes: \n",
    "                self.update_history(node, C)\n",
    "                \n",
    "            e_temp, r_temp, s_temp = excited_nodes, ready_to_fire_nodes, resting_nodes \n",
    "            excited_nodes, ready_to_fire_nodes, resting_nodes = self.update_states()\n",
    "            \n",
    "            if e_temp == excited_nodes and r_temp == ready_to_fire_nodes and s_temp == resting_nodes: \n",
    "                tolerence = tolerence + 1\n",
    "                \n",
    "            self.time = self.time + 1   \n",
    "            \n",
    "        if len(size_of_contagion) < TIME:\n",
    "            if tolerence == 5:\n",
    "                for j in range(len(size_of_contagion), TIME):\n",
    "                    size_of_contagion.append(size_of_contagion[-1])\n",
    "            elif np.any(activation_times==TIME):\n",
    "                for j in range(len(size_of_contagion), TIME):\n",
    "                    size_of_contagion.append(0) \n",
    "            else:\n",
    "                for j in range(len(size_of_contagion), TIME):\n",
    "                    size_of_contagion.append(self.N)\n",
    "        return(activation_times, np.array(size_of_contagion))\n",
    "            \n",
    "    def stack_histories(self, TIME):\n",
    "        \"\"\"\n",
    "        Helper method to stack the histories of all the nodes in the network. Also, resizes the lengths of the\n",
    "        histories from 0 to `TIME`. Call after running the dynamic. Note that only useful if you are running\n",
    "        individual experiments.\n",
    "        \n",
    "        Parameters\n",
    "        =============\n",
    "        TIME: int\n",
    "            The number of time steps that the experiment is going to run for.\n",
    "        \n",
    "        Returns\n",
    "        ==========\n",
    "        all_history: array, of size `N X TIME`\n",
    "            All of the histories of the neurons vertically stack together.\n",
    "        \n",
    "        \"\"\"\n",
    "        for node in self.nodes:\n",
    "            node.history = node.history[:TIME]\n",
    "        states = [node.history for node in self.nodes]\n",
    "        all_history = np.vstack(states)\n",
    "        return(all_history)\n",
    "    \n",
    "    def average_over_trials(self, seed, TIME, C, trials):\n",
    "        \"\"\"\n",
    "        Iterates the `run_dynamic` over `trials` and then takes the average of the first activation times and\n",
    "        contagion size over `trials`.\n",
    "        \n",
    "        Parameters\n",
    "        ===========\n",
    "        seed: int\n",
    "            Node id of the node that the contagion is going to start from.\n",
    "            \n",
    "        TIME: int\n",
    "            The number of time steps that the experiment is going to run for.\n",
    "            \n",
    "        C: int\n",
    "            Curvature of the sigmoid.\n",
    "            \n",
    "        trials: int\n",
    "            Number of times that the experiment is going to be repeated and will be taken the average over.\n",
    "            \n",
    "        Returns\n",
    "        ==========\n",
    "        average_excitation_times: array of size `1 x N`\n",
    "            Array keeping track of the first times that a given node is activated on average i.e. duration \n",
    "            it takes for spread starting from the seed to reach every other node on average over `trials`.\n",
    "            \n",
    "        average_contagion_size: array of size `1 x TIME`\n",
    "            Array saving the length of active nodes at every time step on average `trials`.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        first_excitation_times = np.zeros((self.N, trials))\n",
    "        size_of_contagion = np.zeros((TIME, trials))\n",
    "        for i in range(trials):\n",
    "            first_exct, contagion_size = self.run_dynamic(seed, TIME, C)\n",
    "                                                            \n",
    "            first_excitation_times[:,i] = first_exct\n",
    "            size_of_contagion[:,i] = contagion_size\n",
    "        \n",
    "        average_excitation_times = np.mean(first_excitation_times, axis = 1)\n",
    "        average_contagion_size = np.mean(size_of_contagion, axis = 1)\n",
    "        return(average_excitation_times, average_contagion_size)\n",
    "    \n",
    "    def make_distance_matrix(self, TIME, C, trials):\n",
    "        \"\"\"\n",
    "        The main function above the water that wraps everything together to the user. Runs the experiment over \n",
    "        trials for contagions starting from every node i one by one producing an activation matrix that encodes\n",
    "        the activation times of the nodes j for experiments starting from node i. Then, computes the distances\n",
    "        between the rows of this matrix creating a distance matrix. Also, captures the contagion sizes for every\n",
    "        experiment starting from different nodes.\n",
    "        \n",
    "        Parameters\n",
    "        ==============\n",
    "        TIME: int\n",
    "            The number of time steps that the experiment is going to run for.\n",
    "            \n",
    "        C: int\n",
    "            Curvature of the sigmoid.\n",
    "            \n",
    "        trials: int\n",
    "            Number of times that the experiment is going to be repeated and will be taken the average over.\n",
    "            \n",
    "        Returns\n",
    "        ===========\n",
    "        distance_matrix: array of size `self.N x self.N`\n",
    "            Distance between nodes of the network in terms of the activations.\n",
    "        \n",
    "        Q: array of size `self.N x TIME`\n",
    "            Array where each row i containing the size of the contagion starting from node i at each time step.\n",
    "        \n",
    "        \"\"\"\n",
    "        D = np.zeros((self.N, self.N))\n",
    "        Q = np.zeros((self.N, TIME))\n",
    "        \n",
    "        for seed in range(self.N):\n",
    "            D[seed], Q[seed,:] = self.average_over_trials(seed, TIME, C, trials)\n",
    "        \n",
    "        distance_matrix = euclidean_distances(D.T)\n",
    "        \n",
    "        return(distance_matrix, Q)\n",
    "    \n",
    "    def spy_distance_matrix(self, distance_matrix):\n",
    "        \"\"\"\n",
    "        Helper function to visualize the distance matrix.\n",
    "        \n",
    "        Parameters\n",
    "        =============\n",
    "        distance_matrix: array, `self.N x self.N`\n",
    "            Distance matrix obtained from `make_distance_matrix`.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        fig,ax = plt.subplots(1,1, figsize = (15*int(self.N/20),10*int(self.N/20)))\n",
    "        pos = ax.imshow(distance_matrix, origin = 'lower', interpolation = 'nearest', \n",
    "                        aspect = 'auto', cmap = 'viridis', extent = [0.5,self.N-0.5,0.5,self.N-0.5])\n",
    "        ax.set_title('Distance Matrix for %s'%self.text, fontsize = 20)\n",
    "        ax.set_ylabel('Excitation starting from node i', fontsize = 15)\n",
    "        ax.set_xlabel('First time node j gets activated', fontsize = 15)\n",
    "        ax.set_xticks([i*20 for i in range(int(self.N/20)+1)])\n",
    "        ax.set_yticks([i*20 for i in range(int(self.N/20)+1)])\n",
    "\n",
    "        fig.colorbar(pos, ax = ax)\n",
    "\n",
    "    \n",
    "    def compute_persistence(self, distances, dimension = 2, spy = False):\n",
    "        \"\"\"\n",
    "        Function for computing the persistent homology from the given distance matrix. Once the birth and death\n",
    "        times are calculated, we look at the difference between the lifetimes of two longest living cycles of given\n",
    "        dimension as well as the lifetimes of longest and shortest living cycles which should recover the \n",
    "        `self.manifold`.\n",
    "        \n",
    "        Parameters\n",
    "        ============\n",
    "        \n",
    "        distances: array , `self.N x self.N`\n",
    "            Distance matrix obtained from `make_distance_matrix`.\n",
    "            \n",
    "        dimension: int\n",
    "            Maximum dimension that the simplicial complex will be built. Accordingly, maximum dimensional \n",
    "            topological features are going to be `dimension -1`. Default is 2 suitable for the `Ring` topology, \n",
    "            which is a 1-D compact manifold, but should set to 3 if the topology is a 2D-compact manifold.\n",
    "\n",
    "        spy: bool\n",
    "            If True, persistence diagram will be shown.\n",
    "            \n",
    "        Returns\n",
    "        ===========\n",
    "        \n",
    "        Delta_min: float\n",
    "            Normalized difference between the lifetimes of longest and and second longest living cycles of \n",
    "            dimension `dimension -1`.\n",
    "        Delta_max: float\n",
    "            Normalized difference between the lifetimes of longest and shorthes living cycles of dimension\n",
    "            `dimension -1`.\n",
    "        \"\"\"\n",
    "        rips_complex = gudhi.RipsComplex(distance_matrix = distances/np.max(distances), max_edge_length = 1)\n",
    "        simplex_tree = rips_complex.create_simplex_tree(max_dimension = dimension)\n",
    "        persistence = simplex_tree.persistence(min_persistence = 0.0)\n",
    "        \n",
    "        if spy: \n",
    "            gudhi.plot_persistence_diagram(persistence)\n",
    "\n",
    "        oned_holes = [(0,0)]\n",
    "        for i in range(len(persistence)):\n",
    "            if persistence[i][0] == int(dimension-1):\n",
    "                oned_holes.append(persistence[i][1])\n",
    "        oned_holes = np.array(oned_holes)\n",
    "        persistence_life_times = oned_holes[:,1]-oned_holes[:,0]\n",
    "        Delta_min = np.sort(persistence_life_times)[-1]-np.sort(persistence_life_times)[-2]\n",
    "        Delta_max = np.sort(persistence_life_times)[-1]-np.sort(persistence_life_times)[1]\n",
    "        return(Delta_min, Delta_max)\n",
    "    \n",
    "    def display_comm_sizes(self, Q, labels, TIME = None, C = None, trials = None, memory = None, rest = None):\n",
    "        \"\"\"\n",
    "        Helper function to visualize the size of the active component at each time step.\n",
    "        \n",
    "        Parameters\n",
    "        =============\n",
    "        Q: list\n",
    "            List of community sizes, second output of the `make_distance_matrix`.\n",
    "        labels: list\n",
    "            List of strings that are labels to be used on the plot (Currently set to floats indicating the \n",
    "            global threshold for all neurons in the network.).\n",
    "        \"\"\"\n",
    "        \n",
    "        argmaxs = []\n",
    "        colors = ['violet', 'green', 'black', 'lime', 'blue', 'orange', 'brown', 'yellow', 'red', 'turquoise', \n",
    "                  'purple']\n",
    "    \n",
    "        for i in range(len(Q)):\n",
    "            Q_mean = np.mean(Q[i], axis = 0)\n",
    "            argmaxs.append(np.argmax(Q_mean))\n",
    "    \n",
    "        fig,ax = plt.subplots(1,1, figsize = (20,20))\n",
    "    \n",
    "        for i in range(len(Q)):\n",
    "            Q_mean = np.mean(Q[i], axis = 0)\n",
    "        \n",
    "            if i == 0: ax.plot(Q_mean[:int(np.min([TIME-2,np.max(argmaxs)])+2)], \n",
    "                               label = 'threshold = %.2f'%labels[i], \n",
    "                               linestyle = 'dashed', \n",
    "                               marker = 'v',\n",
    "                               color = colors[i%11])\n",
    "            \n",
    "            else: ax.plot(Q_mean[:int(np.min([TIME-2,np.max(argmaxs)])+2)], \n",
    "                          label = 'threshold = %.2f'%labels[i], \n",
    "                          marker = 'v', \n",
    "                          color = colors[i%11])\n",
    "        \n",
    "            X = np.linspace(0, int(np.min([TIME-2,np.max(argmaxs)])+1), int(np.min([TIME-2,np.max(argmaxs)])+2))\n",
    "            ax.fill_between(X, \n",
    "                            np.max(Q[i], axis = 0)[:int(np.min([TIME-2,np.max(argmaxs)])+2)], \n",
    "                            np.min(Q[i], axis = 0)[:int(np.min([TIME-2,np.max(argmaxs)])+2)], \n",
    "                            alpha = 0.2, color = colors[i%11])\n",
    "        \n",
    "        ax.set_title('%s, T = %d, C = %d, trials = %d,  MEMORY = %d, REST = %d'%(self.text, TIME, C, trials, memory, rest), fontsize = 25)\n",
    "\n",
    "\n",
    "        ax.set_xlabel('Time', fontsize = 20)\n",
    "        ax.set_ylabel('Number of Active Nodes', fontsize = 20)\n",
    "        ax.legend(fontsize = 'large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
